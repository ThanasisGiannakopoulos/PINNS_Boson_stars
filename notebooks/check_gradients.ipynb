{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289417b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec973aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb\n",
    "class FCN(torch.nn.Module):    \n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.Tanh\n",
    "        \n",
    "        self.fcs = torch.nn.Sequential(*[\n",
    "                        torch.nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        self.fch = torch.nn.Sequential(*[\n",
    "                        torch.nn.Sequential(*[\n",
    "                            torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = torch.nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "\n",
    "      # Apply custom weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        act = torch.nn.Softplus()\n",
    "        x = act(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Get the size of the previous layer (input size of the current layer)\n",
    "            n = m.in_features # The number of input features to this layer\n",
    "            # Set the range for uniform distribution as [-1/sqrt(n), 1/sqrt(n)]\n",
    "            bound = 1 / np.sqrt(n)\n",
    "            # Initialize weights with a uniform distribution in the range [-bound, bound]\n",
    "            torch.nn.init.uniform_(m.weight, -bound, bound)\n",
    "            \n",
    "            # Initialize biases to zero, only if the layer has biases\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a58ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "def gradients(outputs, inputs, order = 1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1)\n",
    "    else:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b054636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random (uniform) sample points in (0,1).\n",
    "def random_domain_points(n):\n",
    "    #x = torch.rand((n,1), requires_grad=True)\n",
    "    xhigh = 0.5*torch.rand((int(n/2),1), requires_grad=True) + 0.5 # [0.5,1)\n",
    "    xlow  = -0.5*torch.rand((int(n/2),1), requires_grad=True) + 0.5 # (0,0.5]\n",
    "    x = torch.cat((xlow, xhigh),0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad097c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dir = \"../models/GPU_non_compact_r_fixed_omega/\"\n",
    "dir = dir + \"neurons256_h_layers4_rmax1000_n2000_sigma0.1/\"\n",
    "\n",
    "neurons = 256\n",
    "layers = 4\n",
    "\n",
    "model = FCN(1,4,neurons, layers)#.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b22751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcs.0.weight: grad norm = 1.01e-03\n",
      "fcs.0.bias: grad norm = 1.35e-03\n",
      "fch.0.0.weight: grad norm = 2.49e-02\n",
      "fch.0.0.bias: grad norm = 1.63e-03\n",
      "fch.1.0.weight: grad norm = 2.89e-02\n",
      "fch.1.0.bias: grad norm = 1.93e-03\n",
      "fch.2.0.weight: grad norm = 7.72e-02\n",
      "fch.2.0.bias: grad norm = 4.94e-03\n",
      "fce.weight: grad norm = 6.01e+00\n",
      "fce.bias: grad norm = 3.85e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12677/2763666089.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(dir + \"model_epoch200000.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# test loss\n",
    "\n",
    "# 1. Load your model and switch to train mode (to enable gradients)\n",
    "model.load_state_dict(torch.load(dir + \"model_epoch200000.pth\", map_location=device))\n",
    "model.train()  # important for gradient computation\n",
    "\n",
    "# 2. Prepare input tensor with requires_grad=True\n",
    "# Example for 1D input (time t)\n",
    "x = torch.linspace(0, 1, 100).unsqueeze(1)  # shape (100, 1)\n",
    "x.requires_grad_(True)\n",
    "\n",
    "# 3. Forward pass\n",
    "u = model(x)\n",
    "\n",
    "# 4. Define your loss (example: dummy loss, replace with your residual loss)\n",
    "# For example, if your ODE residual is R(u, x), define:\n",
    "# loss = torch.mean(R(u, x)**2)\n",
    "# Here, we just use a dummy loss: mean squared of u to keep it simple\n",
    "loss = torch.mean(u**2)\n",
    "\n",
    "# 5. Zero existing gradients (good practice)\n",
    "model.zero_grad()\n",
    "\n",
    "# 6. Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# 7. Check and print gradient norms for all parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad norm = {param.grad.norm().item():.2e}\")\n",
    "    else:\n",
    "        print(f\"{name}: no grad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4f48ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcs.0.weight: param = tensor([[-6.7740e-10],\n",
      "        [ 1.9592e-07],\n",
      "        [ 3.2176e-08],\n",
      "        [-4.2725e-06],\n",
      "        [ 3.1326e-09],\n",
      "        [-1.5827e-07],\n",
      "        [-5.9647e-08],\n",
      "        [ 2.4378e-07],\n",
      "        [-2.4818e-09],\n",
      "        [-2.2968e-06],\n",
      "        [-1.3508e-08],\n",
      "        [-1.8403e-08],\n",
      "        [ 2.7657e-09],\n",
      "        [-8.9091e-07],\n",
      "        [-3.2996e-06],\n",
      "        [ 2.2053e-09],\n",
      "        [ 2.5121e-08],\n",
      "        [-1.6397e-08],\n",
      "        [ 8.8085e-08],\n",
      "        [-6.9442e-07],\n",
      "        [-4.0941e-10],\n",
      "        [-3.5903e-07],\n",
      "        [-4.6792e-09],\n",
      "        [-3.0208e-06],\n",
      "        [ 1.7567e-06],\n",
      "        [-7.4281e-08],\n",
      "        [-7.1025e-09],\n",
      "        [-5.4649e-10],\n",
      "        [-1.7954e-09],\n",
      "        [ 9.6616e-08],\n",
      "        [-2.3885e-06],\n",
      "        [-1.0222e-07],\n",
      "        [-3.6752e-09],\n",
      "        [-3.2044e-05],\n",
      "        [ 2.9035e-07],\n",
      "        [ 2.7330e-09],\n",
      "        [ 4.4044e-07],\n",
      "        [ 5.0226e-07],\n",
      "        [-3.1213e-04],\n",
      "        [ 2.7047e-04],\n",
      "        [-7.3231e-05],\n",
      "        [ 1.5957e-08],\n",
      "        [ 1.4310e-08],\n",
      "        [ 2.4886e-09],\n",
      "        [-1.1950e-09],\n",
      "        [ 3.3713e-07],\n",
      "        [-2.8633e-07],\n",
      "        [-3.2561e-09],\n",
      "        [-2.4368e-09],\n",
      "        [-7.0876e-07],\n",
      "        [-5.0565e-05],\n",
      "        [ 1.9269e-05],\n",
      "        [ 1.3205e-07],\n",
      "        [ 2.3531e-09],\n",
      "        [-1.4857e-07],\n",
      "        [ 1.4940e-09],\n",
      "        [ 2.0960e-06],\n",
      "        [ 6.2843e-08],\n",
      "        [ 4.3241e-08],\n",
      "        [ 1.4813e-07],\n",
      "        [-1.5097e-08],\n",
      "        [-6.8227e-09],\n",
      "        [ 1.3003e-09],\n",
      "        [-2.1776e-08],\n",
      "        [-1.0158e-05],\n",
      "        [ 1.4102e-09],\n",
      "        [ 1.9058e-07],\n",
      "        [-4.7950e-08],\n",
      "        [ 2.7839e-06],\n",
      "        [-2.5378e-08],\n",
      "        [ 1.7820e-10],\n",
      "        [-8.6851e-10],\n",
      "        [ 1.1012e-04],\n",
      "        [-1.4253e-04],\n",
      "        [-1.0716e-04],\n",
      "        [ 8.8628e-05],\n",
      "        [ 1.8122e-07],\n",
      "        [-1.7686e-04],\n",
      "        [-1.1239e-07],\n",
      "        [ 6.4982e-09],\n",
      "        [ 9.8224e-09],\n",
      "        [-1.3599e-06],\n",
      "        [-1.5843e-06],\n",
      "        [ 7.9341e-08],\n",
      "        [ 9.4686e-06],\n",
      "        [ 1.5952e-08],\n",
      "        [-6.9826e-06],\n",
      "        [-1.7058e-06],\n",
      "        [ 3.5513e-05],\n",
      "        [ 6.6891e-05],\n",
      "        [-6.0739e-09],\n",
      "        [-5.1835e-09],\n",
      "        [ 2.9438e-08],\n",
      "        [ 7.8744e-09],\n",
      "        [-1.7566e-04],\n",
      "        [ 3.1526e-09],\n",
      "        [ 6.1809e-05],\n",
      "        [ 1.6487e-07],\n",
      "        [-8.0368e-07],\n",
      "        [ 5.0244e-07],\n",
      "        [ 1.9970e-06],\n",
      "        [-1.3236e-06],\n",
      "        [ 1.7567e-07],\n",
      "        [ 7.6584e-10],\n",
      "        [ 1.1149e-08],\n",
      "        [-3.0175e-07],\n",
      "        [-5.0999e-09],\n",
      "        [-1.5697e-07],\n",
      "        [-1.9389e-08],\n",
      "        [-1.8476e-08],\n",
      "        [ 5.9750e-06],\n",
      "        [-4.9983e-10],\n",
      "        [ 2.8518e-04],\n",
      "        [ 5.3796e-09],\n",
      "        [ 3.8302e-08],\n",
      "        [ 3.0585e-06],\n",
      "        [-2.4530e-09],\n",
      "        [ 4.6820e-06],\n",
      "        [-7.2813e-08],\n",
      "        [ 2.5099e-07],\n",
      "        [ 5.6694e-08],\n",
      "        [-3.8896e-04],\n",
      "        [-3.0162e-10],\n",
      "        [-1.4568e-06],\n",
      "        [ 2.3509e-06],\n",
      "        [-9.6294e-09],\n",
      "        [-2.7615e-06],\n",
      "        [ 1.1989e-07],\n",
      "        [ 5.5474e-06],\n",
      "        [ 4.8615e-08],\n",
      "        [ 1.2626e-08],\n",
      "        [ 1.5269e-06],\n",
      "        [-1.2471e-08],\n",
      "        [-4.0258e-06],\n",
      "        [ 2.5453e-09],\n",
      "        [-5.7198e-09],\n",
      "        [-1.3683e-05],\n",
      "        [-3.2882e-08],\n",
      "        [-2.4611e-07],\n",
      "        [-9.2438e-06],\n",
      "        [ 1.2389e-08],\n",
      "        [ 6.5833e-07],\n",
      "        [ 7.3374e-07],\n",
      "        [ 5.5390e-09],\n",
      "        [ 1.5810e-09],\n",
      "        [-1.1865e-07],\n",
      "        [ 5.8190e-06],\n",
      "        [ 3.4749e-08],\n",
      "        [ 2.4481e-09],\n",
      "        [-1.1531e-10],\n",
      "        [ 1.5588e-07],\n",
      "        [-5.5449e-06],\n",
      "        [-1.2917e-05],\n",
      "        [ 1.1865e-07],\n",
      "        [-5.4439e-07],\n",
      "        [-8.5785e-07],\n",
      "        [ 1.5181e-09],\n",
      "        [ 1.3566e-10],\n",
      "        [-1.9506e-08],\n",
      "        [-1.2556e-06],\n",
      "        [-8.3306e-05],\n",
      "        [-4.1334e-07],\n",
      "        [ 8.1160e-08],\n",
      "        [ 4.5491e-04],\n",
      "        [ 2.7885e-09],\n",
      "        [-7.8449e-06],\n",
      "        [ 5.0965e-10],\n",
      "        [-3.2344e-10],\n",
      "        [ 2.2363e-06],\n",
      "        [-5.9284e-08],\n",
      "        [-3.2467e-09],\n",
      "        [-8.3346e-10],\n",
      "        [-5.7265e-05],\n",
      "        [ 2.8090e-08],\n",
      "        [ 4.3156e-10],\n",
      "        [ 3.2857e-10],\n",
      "        [ 2.7522e-07],\n",
      "        [-5.6234e-09],\n",
      "        [ 1.2050e-08],\n",
      "        [-3.8815e-09],\n",
      "        [-2.7699e-09],\n",
      "        [-6.5394e-05],\n",
      "        [-3.0089e-09],\n",
      "        [ 1.8035e-08],\n",
      "        [ 2.4407e-05],\n",
      "        [-2.2952e-08],\n",
      "        [ 6.0773e-09],\n",
      "        [-5.3077e-07],\n",
      "        [ 8.1056e-08],\n",
      "        [-7.1390e-10],\n",
      "        [-1.3508e-07],\n",
      "        [ 2.7392e-09],\n",
      "        [-1.0273e-09],\n",
      "        [-2.1793e-08],\n",
      "        [-5.8242e-10],\n",
      "        [ 2.9498e-09],\n",
      "        [ 6.8932e-10],\n",
      "        [-7.0442e-05],\n",
      "        [ 6.4698e-07],\n",
      "        [ 1.9421e-09],\n",
      "        [-8.2879e-09],\n",
      "        [-1.4484e-06],\n",
      "        [ 2.2094e-04],\n",
      "        [ 1.2295e-07],\n",
      "        [-2.1836e-07],\n",
      "        [-2.1056e-09],\n",
      "        [-2.0191e-06],\n",
      "        [ 1.1026e-08],\n",
      "        [ 8.1002e-08],\n",
      "        [-1.0010e-06],\n",
      "        [ 2.2920e-06],\n",
      "        [-2.3645e-08],\n",
      "        [-1.1842e-04],\n",
      "        [ 2.1412e-06],\n",
      "        [ 1.1407e-06],\n",
      "        [-4.4686e-09],\n",
      "        [ 1.9630e-06],\n",
      "        [-8.7659e-07],\n",
      "        [-3.2353e-09],\n",
      "        [-1.7170e-07],\n",
      "        [ 1.7679e-06],\n",
      "        [ 1.8770e-08],\n",
      "        [-1.8876e-07],\n",
      "        [ 1.8996e-09],\n",
      "        [ 1.0639e-04],\n",
      "        [ 1.1119e-08],\n",
      "        [ 1.4297e-04],\n",
      "        [-6.9365e-09],\n",
      "        [ 2.4251e-09],\n",
      "        [ 9.2007e-10],\n",
      "        [ 2.1792e-05],\n",
      "        [-2.2258e-07],\n",
      "        [ 1.5395e-05],\n",
      "        [-9.1009e-07],\n",
      "        [ 3.3838e-08],\n",
      "        [-4.9532e-05],\n",
      "        [ 8.4081e-08],\n",
      "        [-4.7682e-06],\n",
      "        [ 4.8748e-09],\n",
      "        [-3.5298e-06],\n",
      "        [-2.3594e-08],\n",
      "        [ 7.7348e-07],\n",
      "        [-8.9812e-09],\n",
      "        [-4.5834e-08],\n",
      "        [-3.3814e-09],\n",
      "        [ 3.0716e-09],\n",
      "        [-9.1057e-11],\n",
      "        [ 1.8220e-06],\n",
      "        [ 4.0289e-04],\n",
      "        [ 3.2754e-08],\n",
      "        [ 1.0832e-10],\n",
      "        [ 1.5317e-07],\n",
      "        [-1.0285e-05],\n",
      "        [ 1.3244e-10],\n",
      "        [-2.4585e-09],\n",
      "        [-1.1236e-07]])\n",
      "fcs.0.bias: param = tensor([-8.0894e-10,  2.9649e-07,  4.5003e-08, -5.9542e-06,  3.9120e-09,\n",
      "        -2.2520e-07, -2.4684e-07,  3.0543e-07, -3.4513e-09, -3.2040e-06,\n",
      "        -2.0310e-08, -2.4843e-08,  3.7472e-09, -1.2722e-06, -6.3062e-06,\n",
      "         3.0935e-09,  3.7810e-08, -2.1830e-08,  9.1988e-08, -9.8134e-07,\n",
      "        -5.8300e-10, -4.7863e-07, -6.2567e-09, -4.2002e-06,  2.3732e-06,\n",
      "        -1.0919e-07, -1.1783e-08, -7.9190e-10, -2.1025e-09,  1.2138e-07,\n",
      "        -3.3426e-06, -1.3008e-07, -4.7605e-09, -3.8925e-05,  2.0059e-06,\n",
      "         4.2195e-09,  5.2090e-07,  7.3227e-07, -4.6356e-04,  3.1137e-04,\n",
      "        -8.7089e-05,  2.2049e-08,  1.0330e-08,  4.2563e-09, -1.7758e-09,\n",
      "         5.0030e-07, -4.0956e-07, -4.2694e-09, -1.2439e-08, -8.9555e-07,\n",
      "        -9.3595e-05,  3.8175e-05,  1.7959e-07,  3.1664e-09, -1.6775e-07,\n",
      "         1.7629e-09,  3.0674e-06,  8.5072e-08,  5.9890e-08,  2.2241e-07,\n",
      "        -2.4187e-08, -9.6379e-09, -3.3379e-09, -2.9972e-08, -3.4105e-05,\n",
      "         2.0267e-09,  2.5690e-07, -6.2925e-08,  4.8924e-06, -1.2025e-08,\n",
      "         3.5534e-10, -1.8828e-09,  1.5200e-04, -1.8070e-04, -1.2471e-04,\n",
      "         1.2614e-04,  2.8873e-07, -3.0076e-04, -1.7184e-07,  1.0592e-08,\n",
      "         1.4143e-08, -2.2805e-06, -2.3371e-06,  1.1373e-07,  1.3430e-05,\n",
      "         2.0988e-08, -9.5796e-06, -2.3508e-06,  5.1493e-05,  7.4753e-05,\n",
      "        -8.4378e-09, -7.7487e-09,  4.3010e-08,  1.6376e-08, -2.2049e-04,\n",
      "         4.8332e-09,  1.3930e-04,  2.1963e-07, -1.0843e-06,  9.1455e-07,\n",
      "         1.1595e-05, -8.6455e-06,  2.3332e-07,  1.1426e-09,  1.6548e-08,\n",
      "        -4.1398e-07, -7.2694e-09, -1.0528e-07, -2.7359e-08, -2.3662e-08,\n",
      "         6.4422e-06, -7.8165e-10,  3.3613e-04,  7.2529e-09,  6.0552e-08,\n",
      "         3.6694e-06, -3.6776e-09,  6.6237e-06, -1.9973e-07,  3.1313e-07,\n",
      "         7.3793e-08, -5.0519e-04, -2.9754e-10, -9.0331e-06,  3.5498e-06,\n",
      "        -1.4219e-08, -3.7407e-06,  1.8502e-07,  7.5224e-06,  6.3391e-08,\n",
      "         2.0586e-08,  4.2380e-07, -1.7650e-08, -5.2720e-06,  3.4850e-09,\n",
      "        -8.1037e-09, -4.7677e-05, -4.0239e-08, -4.5396e-07, -1.3611e-05,\n",
      "         1.7186e-08,  8.5159e-07,  1.0139e-06,  7.8075e-09,  2.5097e-09,\n",
      "        -1.6979e-07,  7.6192e-06,  4.8579e-08,  3.4271e-09, -1.8618e-10,\n",
      "         6.5088e-07, -7.6422e-06, -1.8814e-05,  1.1705e-07, -7.6598e-07,\n",
      "        -6.5196e-06,  1.8793e-09,  2.5380e-10, -3.0635e-08, -8.4269e-06,\n",
      "        -1.0326e-04, -5.3531e-07,  1.1674e-07,  5.6808e-04,  3.6997e-09,\n",
      "        -1.3286e-05,  7.7472e-10, -1.5429e-10,  4.6467e-06, -6.3489e-07,\n",
      "        -4.9732e-09, -1.3344e-09, -7.1588e-05,  3.8755e-08,  1.0230e-09,\n",
      "         2.2070e-09,  4.0083e-07, -7.8167e-09,  1.7467e-08, -7.9138e-09,\n",
      "        -4.2740e-09, -8.3094e-05, -4.1608e-09,  2.6072e-08,  3.7501e-05,\n",
      "        -2.7424e-08,  2.6441e-08, -7.3497e-07,  1.2074e-07, -1.0183e-09,\n",
      "        -1.5378e-07,  3.7258e-09, -1.4215e-09, -3.1348e-08, -1.2775e-09,\n",
      "         3.7187e-09,  6.2021e-10, -1.5748e-04,  8.6746e-07,  2.8134e-09,\n",
      "        -9.9823e-09, -2.2192e-06,  2.6864e-04,  1.7954e-07, -1.8021e-07,\n",
      "        -3.0071e-09, -2.7889e-06,  1.5646e-08,  1.1948e-07, -1.3752e-06,\n",
      "         3.2198e-06, -3.5506e-08, -1.7790e-04,  3.2728e-06,  1.6110e-06,\n",
      "        -7.0253e-09,  2.6263e-06, -1.5072e-06, -4.8022e-09, -3.5032e-07,\n",
      "         2.3400e-06,  2.2564e-08, -3.9881e-07,  3.0330e-09,  1.5696e-04,\n",
      "         1.4699e-08,  1.7772e-04, -1.0002e-08,  3.4299e-09,  9.3489e-10,\n",
      "         2.9842e-05,  9.8858e-07,  1.7862e-05, -1.3559e-06,  3.2974e-08,\n",
      "        -1.1878e-04,  8.3805e-08, -6.8097e-06,  7.1289e-09, -5.2177e-06,\n",
      "        -3.2620e-08,  1.0833e-06, -1.7196e-08, -6.5290e-08, -8.1677e-09,\n",
      "         5.6641e-09, -3.0459e-10,  2.5620e-06,  5.6790e-04,  5.0067e-08,\n",
      "         2.3492e-10,  4.2369e-07, -1.3384e-05,  3.3393e-10, -2.9209e-09,\n",
      "         8.0177e-08])\n",
      "fch.0.0.weight: param = tensor([[-5.0926e-09, -5.0658e-09, -5.0897e-09,  ...,  5.0926e-09,\n",
      "         -5.0924e-09, -4.9678e-09],\n",
      "        [-1.4442e-08, -1.4366e-08, -1.4434e-08,  ...,  1.4442e-08,\n",
      "         -1.4442e-08, -1.4088e-08],\n",
      "        [-2.8700e-04, -2.8550e-04, -2.8684e-04,  ...,  2.8701e-04,\n",
      "         -2.8699e-04, -2.7998e-04],\n",
      "        ...,\n",
      "        [-1.7154e-08, -1.7064e-08, -1.7144e-08,  ...,  1.7154e-08,\n",
      "         -1.7153e-08, -1.6733e-08],\n",
      "        [-8.4219e-05, -8.3777e-05, -8.4171e-05,  ...,  8.4219e-05,\n",
      "         -8.4216e-05, -8.2155e-05],\n",
      "        [-1.5491e-05, -1.5410e-05, -1.5482e-05,  ...,  1.5491e-05,\n",
      "         -1.5490e-05, -1.5111e-05]])\n",
      "fch.0.0.bias: param = tensor([ 5.0926e-09,  1.4442e-08,  2.8701e-04, -1.2791e-07,  1.0360e-07,\n",
      "         1.6579e-05, -1.9496e-04, -2.7444e-05,  4.4127e-04, -6.6046e-07,\n",
      "         1.9034e-06, -8.0269e-06, -1.6187e-08, -1.3561e-06,  7.0106e-05,\n",
      "        -2.8416e-07,  6.9645e-06, -5.6987e-09, -5.6529e-07,  2.2071e-05,\n",
      "         1.9600e-09,  6.8578e-08,  1.4317e-07, -6.8308e-08, -2.8170e-08,\n",
      "        -7.4437e-08,  5.9892e-06, -2.8637e-06, -5.6942e-08, -5.7405e-07,\n",
      "        -1.9201e-04,  3.9982e-06, -2.4513e-06, -1.0267e-06, -1.8563e-08,\n",
      "         2.2353e-04, -6.4822e-08,  1.3713e-06, -5.5653e-04, -3.2989e-04,\n",
      "         1.7653e-07, -3.0028e-08,  4.5987e-07, -2.7027e-04, -3.1449e-07,\n",
      "         6.0434e-07,  5.6305e-08, -2.3827e-06, -3.2165e-09,  3.4778e-04,\n",
      "         2.0296e-06,  1.0151e-05, -4.5846e-04, -1.0239e-04, -1.7922e-07,\n",
      "         3.1323e-06,  4.4008e-07,  2.2981e-04,  8.6372e-13, -1.7491e-04,\n",
      "        -4.6587e-08,  1.1399e-06, -8.1819e-08,  8.7740e-08,  3.0833e-08,\n",
      "         4.4803e-07, -7.8837e-05,  1.5586e-04,  4.9206e-08, -1.3931e-04,\n",
      "        -1.8456e-11, -7.6696e-10, -2.1886e-08,  1.4076e-06,  2.6051e-04,\n",
      "         4.3526e-08, -1.5733e-05,  2.1433e-06,  1.2847e-05, -1.0934e-06,\n",
      "        -1.5650e-07, -1.2293e-07,  6.2823e-05, -2.5518e-06, -2.9155e-04,\n",
      "        -2.1498e-04, -9.0637e-05, -2.1979e-06, -8.0702e-07,  7.1987e-06,\n",
      "         6.9580e-08,  7.6375e-07, -1.7321e-07,  3.2120e-07, -1.2501e-04,\n",
      "        -1.6908e-05,  1.2347e-08,  0.0000e+00, -2.8790e-09,  6.8119e-06,\n",
      "        -1.1704e-06,  8.0054e-08, -2.9845e-09, -1.4285e-07,  2.0607e-08,\n",
      "        -2.7634e-05,  5.6920e-06,  3.4266e-08,  1.5045e-09,  1.1743e-07,\n",
      "         4.4751e-08,  5.9997e-07, -1.7167e-06, -9.2205e-06,  1.2681e-08,\n",
      "         1.0729e-07,  8.4004e-07,  4.4546e-07, -2.1168e-07, -1.1919e-06,\n",
      "         5.0277e-06,  3.7104e-08, -3.0715e-06,  1.2473e-08,  1.1621e-08,\n",
      "         7.0012e-08,  2.3858e-10, -1.8347e-07, -2.6568e-06,  5.0080e-05,\n",
      "         1.7218e-10,  1.0836e-06,  1.3787e-04, -1.5456e-07, -2.6305e-07,\n",
      "        -8.5919e-06, -2.2206e-06,  7.9718e-10,  8.2584e-06,  2.1356e-06,\n",
      "        -2.5306e-05, -2.6285e-05,  3.9730e-05,  2.3314e-06,  4.3798e-07,\n",
      "        -2.5235e-08, -8.3827e-06, -1.0001e-10, -5.5956e-05, -3.6184e-10,\n",
      "         6.4185e-08,  2.4304e-07, -6.6266e-08, -8.9714e-06, -1.7719e-10,\n",
      "        -2.8035e-04, -1.3560e-07,  1.8523e-10, -9.1612e-05, -7.5802e-07,\n",
      "         1.1842e-07,  1.6606e-06,  1.9888e-06,  1.1049e-05, -1.2616e-06,\n",
      "        -4.1968e-07, -1.3726e-06,  6.3197e-06, -9.2983e-08, -1.1806e-10,\n",
      "        -3.4799e-05, -1.7481e-08, -3.3460e-07, -1.1854e-07,  5.2347e-08,\n",
      "         2.5815e-09,  9.7007e-08, -1.1437e-07, -3.2321e-07, -1.2684e-07,\n",
      "        -1.9368e-05,  9.1152e-05, -3.9583e-08,  1.7861e-08,  2.1595e-06,\n",
      "        -4.3249e-06,  7.0730e-05, -4.8955e-05, -3.7841e-07, -2.9592e-06,\n",
      "         4.7784e-07, -3.6165e-08, -4.4562e-08, -5.6180e-07,  2.7708e-08,\n",
      "        -3.3686e-04,  2.5728e-04,  1.6966e-08,  3.4263e-07, -1.7541e-06,\n",
      "        -1.2251e-06,  2.9373e-09, -2.3417e-07,  7.9595e-08,  7.8492e-11,\n",
      "        -1.4299e-05, -1.7377e-05, -2.3424e-06, -2.7573e-06,  3.7823e-06,\n",
      "        -1.2476e-05, -4.4927e-08, -4.0291e-04,  1.3246e-06, -3.0679e-04,\n",
      "        -1.1476e-06,  3.2898e-06,  2.3323e-08,  0.0000e+00, -3.6795e-05,\n",
      "         2.5397e-06, -6.0719e-05, -9.2253e-07, -3.9115e-09, -2.5393e-08,\n",
      "         8.2345e-08, -7.2730e-06,  1.3367e-09,  6.3735e-04, -7.5891e-07,\n",
      "         7.5650e-08,  1.7443e-07, -5.3570e-07,  4.7658e-09,  1.0884e-07,\n",
      "        -4.9184e-07,  1.7544e-07,  1.1226e-07,  2.1672e-06,  8.6772e-12,\n",
      "         3.6241e-08,  2.3169e-07, -3.7671e-07,  8.0360e-05, -1.9457e-07,\n",
      "        -2.0764e-04, -1.1337e-08,  9.3015e-06,  8.1644e-08,  8.9267e-06,\n",
      "        -2.0728e-07,  3.4536e-05, -4.0826e-08,  1.7154e-08,  8.4220e-05,\n",
      "         1.5491e-05])\n",
      "fch.1.0.weight: param = tensor([[ 3.2691e-07,  3.2692e-07, -1.1868e-07,  ...,  3.2690e-07,\n",
      "          8.6153e-08,  3.0780e-07],\n",
      "        [-1.2978e-07, -1.2978e-07,  5.5651e-08,  ..., -1.2977e-07,\n",
      "         -3.0330e-08, -1.2185e-07],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.3375e-06,  1.3376e-06,  5.5763e-08,  ...,  1.3376e-06,\n",
      "          6.1885e-07,  1.2847e-06],\n",
      "        [-1.0307e-07, -1.0307e-07,  3.0180e-08,  ..., -1.0307e-07,\n",
      "         -3.1203e-08, -9.7433e-08],\n",
      "        [ 5.3143e-07,  5.3145e-07, -1.7749e-07,  ...,  5.3143e-07,\n",
      "          1.4850e-07,  5.0115e-07]])\n",
      "fch.1.0.bias: param = tensor([-3.2694e-07,  1.2979e-07,  0.0000e+00,  7.3044e-04,  5.1969e-07,\n",
      "        -7.8236e-07,  1.5265e-08,  4.0897e-07,  3.3132e-07,  0.0000e+00,\n",
      "        -3.7114e-07,  1.8938e-07,  4.7944e-06,  5.6669e-08,  1.0175e-04,\n",
      "         1.2783e-07,  4.5872e-07,  8.8027e-08,  6.3799e-07,  2.7526e-12,\n",
      "        -8.9791e-08,  6.9457e-07,  9.7153e-09,  1.2048e-10, -1.9201e-11,\n",
      "         1.0699e-07, -2.3662e-04, -1.5481e-05,  0.0000e+00,  4.8470e-06,\n",
      "         1.7214e-04, -2.9001e-05,  0.0000e+00,  4.8594e-07, -5.0362e-08,\n",
      "        -1.6013e-04,  0.0000e+00,  4.5401e-08, -8.7447e-08,  6.2571e-08,\n",
      "        -5.1425e-07,  1.5315e-07, -1.5220e-07, -3.7302e-05, -1.0012e-07,\n",
      "        -5.9215e-08, -4.3742e-10,  6.8406e-08, -1.9376e-04,  1.3419e-05,\n",
      "         3.1951e-06, -3.2827e-07,  5.9882e-04, -3.6478e-08, -1.1312e-07,\n",
      "        -2.3785e-08,  3.8569e-08,  1.6671e-07,  1.2212e-10, -5.0638e-07,\n",
      "         3.5154e-07,  6.9917e-08,  4.2636e-07,  4.0040e-07,  1.6043e-08,\n",
      "         4.0043e-08,  1.0995e-07, -8.1146e-05,  6.2130e-08,  1.9536e-06,\n",
      "         3.2761e-10,  3.6091e-07, -3.7824e-08,  1.3703e-10, -4.7359e-08,\n",
      "        -4.3858e-09,  5.5444e-04, -5.5918e-08,  1.3003e-10,  2.7673e-07,\n",
      "         3.3715e-05,  8.0063e-08, -7.7118e-08,  4.4526e-07, -2.8879e-07,\n",
      "        -1.7250e-07, -1.1605e-07,  6.9389e-09, -2.0468e-07,  5.3509e-07,\n",
      "         9.1923e-07,  1.1091e-07, -1.8211e-07, -2.1005e-07, -1.2310e-07,\n",
      "        -2.3878e-06,  9.8476e-08, -5.8256e-11, -4.7556e-11, -2.6939e-08,\n",
      "         5.5232e-08,  3.3316e-07, -1.5786e-05,  1.2279e-07,  3.6608e-08,\n",
      "         1.1645e-08, -3.1318e-05,  1.5637e-06, -6.4170e-08, -6.2210e-11,\n",
      "         1.2787e-07, -3.1494e-09,  2.8153e-12,  9.1063e-07, -2.2793e-08,\n",
      "         5.2654e-07, -1.3923e-07,  2.1913e-08, -1.1297e-06, -1.4730e-06,\n",
      "        -5.4399e-06, -8.3498e-05, -2.1500e-08,  1.8142e-08,  7.7439e-08,\n",
      "         1.4957e-08, -5.3036e-11, -3.7774e-04, -1.1825e-07,  6.4267e-09,\n",
      "         2.6912e-08, -1.1203e-07,  4.1103e-07, -7.2903e-09,  2.2264e-07,\n",
      "         0.0000e+00, -1.5828e-07, -3.3718e-10,  4.4851e-05, -2.1577e-08,\n",
      "         7.0962e-07, -9.7978e-05, -2.1471e-07, -2.5511e-07, -5.2444e-08,\n",
      "         1.0474e-07, -6.3031e-09, -3.5180e-10,  6.4483e-05, -5.6568e-07,\n",
      "        -8.1011e-10, -4.2971e-08,  2.2856e-08, -7.8003e-09, -3.6177e-06,\n",
      "        -8.1089e-09, -4.7048e-08, -2.1650e-08,  0.0000e+00, -3.1517e-06,\n",
      "        -2.0788e-08,  2.3024e-07,  4.9557e-08, -4.7399e-08,  9.1330e-07,\n",
      "        -5.5420e-09,  4.8015e-07,  1.2507e-07,  3.0084e-08,  6.2961e-07,\n",
      "         1.2452e-08,  9.3617e-08,  1.3994e-07, -3.5378e-06,  1.3337e-07,\n",
      "         6.4414e-04, -6.7411e-08, -4.0647e-08,  8.0125e-06, -2.5228e-08,\n",
      "         1.9035e-04,  3.0338e-08,  7.0121e-08,  0.0000e+00,  4.7181e-04,\n",
      "        -4.4827e-07,  4.8873e-08,  4.3097e-06, -3.9163e-07,  8.6238e-08,\n",
      "         8.7128e-08,  1.3133e-09,  3.1495e-08,  1.5796e-08, -4.9427e-08,\n",
      "         8.4841e-08,  6.7484e-07,  1.7943e-06,  1.4624e-07, -1.2573e-08,\n",
      "         3.9285e-08, -1.8088e-04,  6.0229e-07, -2.8005e-07,  5.3694e-05,\n",
      "         2.8203e-07, -6.9259e-07, -1.6289e-07, -5.3308e-09,  1.2497e-10,\n",
      "         2.4993e-07,  8.5570e-04, -1.5549e-08,  1.9972e-09,  0.0000e+00,\n",
      "        -3.5845e-09, -5.0519e-07, -1.0626e-07, -8.4502e-08, -8.8498e-06,\n",
      "         3.1280e-08,  1.8023e-08,  3.7663e-06,  1.4710e-09, -1.1180e-07,\n",
      "        -1.8998e-11, -1.1868e-04, -3.8827e-07, -2.9027e-07, -8.4086e-04,\n",
      "         3.8136e-08, -4.0077e-08,  1.3365e-04,  3.6460e-07,  2.0132e-07,\n",
      "         5.4317e-08, -1.6568e-08, -7.8552e-07,  3.0067e-10,  3.1758e-07,\n",
      "        -8.8143e-08, -1.9051e-07, -1.9259e-08, -1.7220e-08, -7.2066e-08,\n",
      "        -2.2743e-08,  7.0070e-06, -2.9155e-06, -6.7939e-09,  9.5653e-08,\n",
      "         1.2042e-07, -2.6109e-07, -3.7660e-07, -1.3377e-06,  1.0308e-07,\n",
      "        -5.3148e-07])\n",
      "fch.2.0.weight: param = tensor([[-3.2996e-07,  3.2997e-07,  3.3009e-07,  ...,  3.2781e-07,\n",
      "          3.3005e-07,  3.2999e-07],\n",
      "        [ 3.8046e-08, -3.8046e-08, -3.8060e-08,  ..., -3.6194e-08,\n",
      "         -3.8055e-08, -3.8048e-08],\n",
      "        [ 1.7215e-07, -1.7215e-07, -1.7221e-07,  ..., -1.6972e-07,\n",
      "         -1.7219e-07, -1.7216e-07],\n",
      "        ...,\n",
      "        [-1.4699e-08,  1.4699e-08,  1.4704e-08,  ...,  1.4284e-08,\n",
      "          1.4702e-08,  1.4700e-08],\n",
      "        [ 1.9600e-06, -1.9600e-06, -1.9608e-06,  ..., -1.9334e-06,\n",
      "         -1.9605e-06, -1.9602e-06],\n",
      "        [-4.6426e-08,  4.6426e-08,  4.6443e-08,  ...,  4.5296e-08,\n",
      "          4.6437e-08,  4.6429e-08]])\n",
      "fch.2.0.bias: param = tensor([-3.3009e-07,  3.8060e-08,  1.7221e-07, -4.0049e-03, -6.8129e-09,\n",
      "         8.5201e-08,  1.0421e-04, -3.1800e-08, -1.6100e-08,  1.1864e-06,\n",
      "         1.3988e-07,  1.1517e-07,  5.8419e-08, -3.6635e-07, -3.3783e-07,\n",
      "        -3.4712e-08, -4.3129e-06, -5.6772e-08,  1.0244e-07, -7.8911e-09,\n",
      "        -9.3916e-09,  2.7482e-07, -2.6139e-07, -5.6593e-06,  1.9848e-06,\n",
      "         7.9789e-09,  8.4469e-06,  9.6289e-06,  3.3762e-08, -8.9246e-08,\n",
      "        -9.5981e-08, -1.1240e-05,  4.3631e-08, -1.7888e-08, -5.3542e-08,\n",
      "         2.4160e-08, -2.1510e-07, -1.9275e-07,  8.0025e-09, -3.9067e-08,\n",
      "         1.9704e-07,  8.4122e-08,  1.6688e-07, -1.4063e-05,  1.1010e-07,\n",
      "         5.1425e-08, -1.4305e-07, -2.7941e-06,  1.4606e-08, -7.4456e-08,\n",
      "         6.6353e-04,  2.6307e-08,  1.5321e-07,  3.9805e-08, -1.1159e-07,\n",
      "        -1.0952e-07,  4.2648e-04, -2.0500e-07,  6.6328e-08, -5.0500e-08,\n",
      "         5.5260e-08, -1.6058e-07, -1.7396e-08,  7.2154e-09, -5.7887e-04,\n",
      "         2.4120e-08,  3.7589e-08,  7.6679e-08,  6.5479e-09, -1.8289e-08,\n",
      "         7.9191e-08,  5.7111e-08,  2.7365e-08, -2.9192e-08, -2.9471e-06,\n",
      "        -4.9507e-08, -1.0171e-05,  1.3607e-06, -5.3224e-08,  6.5917e-08,\n",
      "         8.0106e-08, -6.6948e-07, -1.6464e-08, -6.6882e-08,  7.0326e-07,\n",
      "         3.0938e-08, -2.7502e-07, -1.9541e-08, -1.5274e-07, -9.5518e-08,\n",
      "         8.3359e-08,  6.4750e-08,  1.2115e-07,  4.4997e-07,  4.2523e-07,\n",
      "        -2.6602e-08, -7.7171e-08, -2.9810e-06,  2.7696e-07,  6.9877e-08,\n",
      "         5.1305e-08,  3.5081e-08,  1.5716e-05,  9.1523e-08,  1.1903e-08,\n",
      "        -1.4687e-07,  2.0007e-08, -6.3647e-08, -5.1431e-08, -2.0955e-08,\n",
      "        -1.2780e-06,  4.0989e-09,  6.2345e-08, -3.5753e-06, -3.5591e-08,\n",
      "         1.1034e-07,  8.6983e-08,  1.1745e-04, -1.0957e-07,  1.3522e-06,\n",
      "         5.3958e-08,  1.5672e-04, -5.4299e-09, -8.5671e-08, -8.3017e-08,\n",
      "         4.7609e-06, -2.5422e-05, -5.4506e-08,  2.0987e-10,  7.0168e-08,\n",
      "         2.4680e-07,  4.7182e-08,  1.6829e-07, -6.2000e-08, -5.2005e-05,\n",
      "        -3.5889e-06,  9.5229e-08, -2.0420e-06,  9.2690e-08,  3.4399e-10,\n",
      "         9.8791e-10, -6.8020e-08, -1.6350e-08, -2.0373e-07,  7.7568e-08,\n",
      "         2.1234e-07, -7.2413e-08, -1.9457e-08,  3.1355e-06,  3.7961e-08,\n",
      "         1.4405e-08, -4.6126e-08, -8.0406e-07,  2.4526e-06,  1.1293e-07,\n",
      "         1.5936e-06, -6.3837e-08, -2.1370e-05, -1.8418e-04,  1.0531e-08,\n",
      "        -1.0026e-07,  6.9615e-06, -2.6474e-07,  2.2804e-07,  6.2900e-08,\n",
      "        -2.3972e-06,  2.0657e-03,  3.0935e-04,  1.7527e-06, -7.9803e-08,\n",
      "         1.3184e-07,  2.3463e-07, -8.2623e-06, -5.1964e-08, -1.3051e-07,\n",
      "         1.2374e-08, -5.1453e-08,  7.7857e-08, -3.3109e-07, -1.5364e-08,\n",
      "         1.8772e-04, -1.2687e-08, -3.9486e-08, -2.8672e-08,  2.7840e-08,\n",
      "         8.6773e-08, -8.2721e-08, -1.4494e-05,  1.7797e-05,  1.3361e-08,\n",
      "        -5.4891e-07,  1.0211e-07,  3.0297e-08, -6.7742e-08,  1.2850e-07,\n",
      "        -2.2282e-08, -1.9496e-07, -4.9428e-06,  2.3505e-08, -3.1485e-07,\n",
      "         2.8108e-07,  2.4956e-07, -5.6061e-08,  1.7939e-07, -6.6668e-06,\n",
      "         1.8710e-07,  2.7115e-04,  7.2705e-08, -2.9386e-06, -6.8525e-08,\n",
      "        -1.9702e-07, -3.9361e-07,  9.8520e-08,  1.2092e-07,  3.3435e-08,\n",
      "         6.5089e-08,  1.1057e-07, -1.0453e-08,  4.3405e-06, -4.4164e-08,\n",
      "        -1.0385e-07, -1.6359e-03,  1.1165e-08,  7.1637e-06,  1.5731e-07,\n",
      "        -1.4561e-07, -1.8904e-05,  7.1757e-08,  7.3661e-10,  4.4360e-04,\n",
      "        -1.4024e-07, -9.0745e-08, -3.0887e-08,  2.7004e-08,  4.5438e-08,\n",
      "        -9.8866e-08,  8.7433e-08,  1.9880e-07, -2.9796e-06, -4.5646e-08,\n",
      "         1.2610e-07, -1.1796e-07,  2.8067e-08, -1.4644e-07, -7.6806e-08,\n",
      "         3.0381e-05, -7.6391e-08,  6.7691e-08, -5.5829e-08, -1.6693e-07,\n",
      "        -1.2406e-06,  1.4533e-07, -7.9052e-05, -1.4704e-08,  1.9608e-06,\n",
      "        -4.6443e-08])\n",
      "fce.weight: param = tensor([[ 3.1989e-01,  3.1990e-01, -3.1990e-01,  ...,  3.1990e-01,\n",
      "          3.1884e-01,  3.1989e-01],\n",
      "        [ 2.1383e-01,  2.1383e-01, -2.1383e-01,  ...,  2.1383e-01,\n",
      "          2.1313e-01,  2.1383e-01],\n",
      "        [ 3.9623e-06,  3.9623e-06, -3.9623e-06,  ...,  3.9623e-06,\n",
      "          3.9434e-06,  3.9623e-06],\n",
      "        [ 1.1704e-03,  1.1704e-03, -1.1704e-03,  ...,  1.1704e-03,\n",
      "          1.1666e-03,  1.1704e-03]])\n",
      "fce.bias: param = tensor([3.1990e-01, 2.1383e-01, 3.9623e-06, 1.1704e-03])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: param = {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c825f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12677/4022050355.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(dir + \"model_epoch200000.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss_f1 gradients:\n",
      "  fcs.0.weight: grad norm = 1.61e-04\n",
      "  fcs.0.bias: grad norm = 3.37e-05\n",
      "  fch.0.0.weight: grad norm = 2.62e-04\n",
      "  fch.0.0.bias: grad norm = 1.71e-05\n",
      "  fch.1.0.weight: grad norm = 3.50e-04\n",
      "  fch.1.0.bias: grad norm = 2.43e-05\n",
      "  fch.2.0.weight: grad norm = 5.26e-03\n",
      "  fch.2.0.bias: grad norm = 3.38e-04\n",
      "  fce.weight: grad norm = 1.12e-02\n",
      "  fce.bias: grad norm = 7.08e-04\n",
      "\n",
      "loss_f2 gradients:\n",
      "  fcs.0.weight: grad norm = 5.31e-05\n",
      "  fcs.0.bias: grad norm = 4.93e-06\n",
      "  fch.0.0.weight: grad norm = 6.88e-05\n",
      "  fch.0.0.bias: grad norm = 4.43e-06\n",
      "  fch.1.0.weight: grad norm = 6.58e-05\n",
      "  fch.1.0.bias: grad norm = 4.65e-06\n",
      "  fch.2.0.weight: grad norm = 3.24e-04\n",
      "  fch.2.0.bias: grad norm = 2.05e-05\n",
      "  fce.weight: grad norm = 1.90e-03\n",
      "  fce.bias: grad norm = 1.25e-04\n",
      "\n",
      "loss_f3 gradients:\n",
      "  fcs.0.weight: grad norm = 1.55e-06\n",
      "  fcs.0.bias: grad norm = 1.75e-07\n",
      "  fch.0.0.weight: grad norm = 3.65e-06\n",
      "  fch.0.0.bias: grad norm = 2.34e-07\n",
      "  fch.1.0.weight: grad norm = 3.47e-06\n",
      "  fch.1.0.bias: grad norm = 2.41e-07\n",
      "  fch.2.0.weight: grad norm = 1.86e-05\n",
      "  fch.2.0.bias: grad norm = 1.08e-06\n",
      "  fce.weight: grad norm = 1.06e-04\n",
      "  fce.bias: grad norm = 6.84e-06\n",
      "\n",
      "loss_f4 gradients:\n",
      "  fcs.0.weight: grad norm = 2.68e-06\n",
      "  fcs.0.bias: grad norm = 3.52e-07\n",
      "  fch.0.0.weight: grad norm = 7.20e-06\n",
      "  fch.0.0.bias: grad norm = 4.66e-07\n",
      "  fch.1.0.weight: grad norm = 8.19e-06\n",
      "  fch.1.0.bias: grad norm = 5.81e-07\n",
      "  fch.2.0.weight: grad norm = 1.24e-04\n",
      "  fch.2.0.bias: grad norm = 8.09e-06\n",
      "  fce.weight: grad norm = 5.47e-05\n",
      "  fce.bias: grad norm = 3.53e-06\n"
     ]
    }
   ],
   "source": [
    "# Assume: loss_f1, loss_f2, loss_f3, loss_f4 represent your 4 ODE residual losses\n",
    "# These should be computed from the same forward pass if possible\n",
    "# If your model outputs a vector [u1, u2, u3, u4], you can separate them as needed\n",
    "\n",
    "# Step 1: Load model\n",
    "model.load_state_dict(torch.load(dir + \"model_epoch200000.pth\", map_location=device))\n",
    "model.train()\n",
    "\n",
    "# Step 2: Input (enable gradients for PINN-style derivatives)\n",
    "r = torch.linspace(0, 30, 1000).unsqueeze(1).to(device).requires_grad_(True)\n",
    "\n",
    "# Step 3: Forward pass\n",
    "u = model(r)  # assume u has shape (100, 4)\n",
    "\n",
    "# Step 4: Split u if needed\n",
    "A, alpha, chi_minus, phi = map(lambda i:  u[:, [i]], range(4))\n",
    "chi = - chi_minus\n",
    "Ar = gradients(A, r)\n",
    "alphar = gradients(alpha, r)\n",
    "chir = gradients(chi, r)\n",
    "phir = gradients(phi, r)\n",
    "\n",
    "omega = 0.895042 * torch.ones(1).to(device)\n",
    "phi0  = 0.05  * torch.ones(1).to(device)\n",
    "m = torch.ones(1).to(device)\n",
    "\n",
    "V = 0.5 * torch.pow(m, 2) * torch.pow(phi, 2)\n",
    "dVdphi = torch.pow(m, 2) * phi\n",
    "rho = 0.5 * (torch.pow(chi, 2) / A + torch.pow((omega / alpha), 2) * torch.pow(phi, 2)) + V\n",
    "SA = 0.5 * (torch.pow(chi, 2) / A + torch.pow((omega / alpha), 2) * torch.pow(phi, 2)) - V\n",
    "eq_A = r * Ar - A * ((1 - A) + 8 * torch.pi * (r ** 2) * A * rho)\n",
    "eq_alpha = r * alphar - alpha * (0.5 * (A - 1) + 8 * torch.pi * A * (r ** 2) * SA)\n",
    "eq_chi = r * chir + (chi) * (1 + A - 8 * torch.pi * r * A * V) - r * A * (dVdphi - torch.pow((omega / alpha), 2) * phi)\n",
    "eq_phi = phir - chi\n",
    "\n",
    "# Step 5: Define your four individual losses (these are just placeholders)\n",
    "# Replace these with your actual physics-informed residuals\n",
    "loss_f1 = torch.mean(torch.pow(eq_A, 2))\n",
    "loss_f2 = torch.mean(torch.pow(eq_alpha, 2))\n",
    "loss_f3 = torch.mean(torch.pow(eq_chi, 2))\n",
    "loss_f4 = torch.mean(torch.pow(eq_phi, 2))\n",
    "\n",
    "# Step 6: For each loss, backward and check grads\n",
    "losses = [loss_f1, loss_f2, loss_f3, loss_f4]\n",
    "loss_names = ['loss_f1', 'loss_f2', 'loss_f3', 'loss_f4']\n",
    "\n",
    "for i, (loss, name) in enumerate(zip(losses, loss_names)):\n",
    "    model.zero_grad()  # Clear gradients before each backward\n",
    "    loss.backward(retain_graph=True)  # retain_graph=True allows reuse of computation\n",
    "    print(f\"\\n{name} gradients:\")\n",
    "    for pname, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"  {pname}: grad norm = {param.grad.norm().item():.2e}\")\n",
    "        else:\n",
    "            print(f\"  {pname}: no grad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592b9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
