{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63973009-c01b-4846-930d-e760883dcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84125085-3cbb-4227-9f14-4cd9d3f9ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('torch version:',torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416793e3-e216-4365-bf97-dbedb5c1fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('device:', device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('CUDA is not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558fed9-e2f4-4faa-8b4e-e8eed8faf470",
   "metadata": {},
   "source": [
    "The system:\n",
    "\n",
    "\\begin{align}\n",
    "\\partial_r A &= A \\left[ \\frac{(1-A)}{r} + 8 \\pi r A \\rho \\right] \\; ,\n",
    " \\\\\n",
    "\\partial_r \\alpha &= \\alpha \\left[ \\frac{(A-1)}{2r} + 8 \\pi r A S_A \\right] \\; ,\n",
    " \\\\\n",
    "\\partial_r \\chi &= - \\frac{\\chi}{r} \\left[ 1+A - 8 \\pi r A V(\\phi) \\right]\n",
    "+ A \\left[ \\rule{0mm}{5mm} \\frac{dV}{d\\phi}\n",
    "- \\left( \\frac{\\omega}{\\alpha} \\right)^2 \\phi \\right]  \\; ,\n",
    " \\\\\n",
    "\\partial_r \\phi &= \\chi\n",
    " \\; ,\n",
    "\\end{align}\n",
    "\n",
    "Boundary conditions:\n",
    "\\begin{align}\n",
    "A(r=0) = 1 \\,, \\quad \\partial_r \\alpha(r=0) = 0 \\,, \\quad \\phi(r=0) = \\phi_0 \\,, \\quad \\chi(r=0) = 0 \\,,\n",
    "\\\\\n",
    "A(r \\rightarrow \\infty) = 1 \\,, \\quad \\alpha(r \\rightarrow \\infty) = 1 \\,, \\quad \\phi(r \\rightarrow \\infty) = 0 \\,, \\quad \\chi(r \\rightarrow \\infty) = 0 \\,.\n",
    "\\end{align}\n",
    "\n",
    "The potential is\n",
    "$$\n",
    "V(\\phi) = \\frac{1}{2} m^2 \\phi^2 \\,, \\quad \\frac{d V}{d \\phi} = m^2 \\phi \\,.\n",
    "$$\n",
    "\n",
    "To start, let's try to get a solution for fixed $\\omega$ and $\\phi_0$ (known solutions from Miguel Bezares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61301be-96fd-4ee5-a69e-e4598ff1d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb\n",
    "class FCN(torch.nn.Module):    \n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.Tanh\n",
    "        \n",
    "        self.fcs = torch.nn.Sequential(*[\n",
    "                        torch.nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        self.fch = torch.nn.Sequential(*[\n",
    "                        torch.nn.Sequential(*[\n",
    "                            torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = torch.nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "\n",
    "      # Apply custom weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Get the size of the previous layer (input size of the current layer)\n",
    "            n = m.in_features # The number of input features to this layer\n",
    "            # Set the range for uniform distribution as [-1/sqrt(n), 1/sqrt(n)]\n",
    "            bound = 1 / np.sqrt(n)\n",
    "            # Initialize weights with a uniform distribution in the range [-bound, bound]\n",
    "            torch.nn.init.uniform_(m.weight, -bound, bound)\n",
    "            \n",
    "            # Initialize biases to zero, only if the layer has biases\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec3044-8aa4-4087-9cbf-b793fa8bc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "def gradients(outputs, inputs, order = 1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1)\n",
    "    else:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3c387-ba15-4f20-a5f2-e3c2cd2237e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random (uniform) sample points in (0,1).\n",
    "def random_domain_points(n):\n",
    "    #x = torch.rand((n,1), requires_grad=True)\n",
    "    xhigh = 0.5*torch.rand((int(n/2),1), requires_grad=True) + 0.5 # [0.5,1)\n",
    "    xlow  = -0.5*torch.rand((int(n/2),1), requires_grad=True) + 0.5 # (0,0.5]\n",
    "    x = torch.cat((xlow, xhigh),0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac0401-6f33-46cd-a1d1-7d7b2c81c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_loss(u, r, omega, m): # eps is epsilon <<1 to never have vanishing denominators \n",
    "    # torch.transpose(ux.detach(), 0, 1) remove some info and transpose, then [0] is Ax, etc\n",
    "    #A = torch.transpose(u.detach(), 0, 1)[0].view(-1,1)\n",
    "    #alpha = torch.transpose(u.detach(), 0, 1)[1].view(-1,1)\n",
    "    #chi = torch.transpose(u.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(u.detach(), 0, 1)[3].view(-1,1)\n",
    "    # output same as above\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  u[:,[i]], range(4))\n",
    "    # take derivatives\n",
    "    Ar = gradients(A, r)\n",
    "    alphar = gradients(alpha, r)\n",
    "    chir = gradients(chi, r)\n",
    "    phir = gradients(phi, r)\n",
    "    \n",
    "    omega = omega.to(device)\n",
    "    \n",
    "    # potential\n",
    "    V = 0.5*torch.pow(m,2)*torch.pow(phi,2)\n",
    "    # potential derivative wrt phi\n",
    "    dVdphi = torch.pow(m,2)*phi\n",
    "    # rho\n",
    "    rho = 0.5*(torch.pow(chi,2)/A + torch.pow((omega/alpha),2)*torch.pow(phi,2)) + V\n",
    "    # S_A\n",
    "    SA = 0.5*(torch.pow(chi,2)/A + torch.pow((omega/alpha),2)*torch.pow(phi,2)) - V\n",
    "    # eq_A is \\p_rA - rhs[A]\n",
    "    eq_A = Ar - A*( (1-A)/r + 8*torch.pi*r*A*rho)\n",
    "    # eq_alpha is \\p_r alpha - rhs[alpha]\n",
    "    eq_alpha = alphar - alpha*( (A-1)/(2*r) + 4*torch.pi*A*r*SA)\n",
    "    # eq_chi  is \\p_r chi - rhs[chi]\n",
    "    eq_chi = chir + (chi/(r))*(1 + A - 8*torch.pi*r*A*V) - (A)*(dVdphi - torch.pow((omega/alpha),2)*phi)\n",
    "    # eq_phi is \\p_r phi - rhs[phi]\n",
    "    eq_phi = phir - chi\n",
    "\n",
    "    loss_dom = (torch.mean(torch.pow(eq_A,2)) + torch.mean(torch.pow(eq_alpha,2))\n",
    "                + torch.mean(torch.pow(eq_chi,2)) + torch.mean(torch.pow(eq_phi,2)) \n",
    "                #+ torch.mean(torch.pow(torch.sqrt(torch.pow(phi,2)) - phi,2))\n",
    "               )\n",
    "               \n",
    "    return loss_dom\n",
    "\n",
    "def r0_loss(u0, r0, phi0):\n",
    "    #alphax = torch.transpose(ux0.detach(), 0, 1)[1].view(-1,1)\n",
    "    #A = torch.transpose(u0.detach(), 0, 1)[0].view(-1,1)\n",
    "    #chi = torch.transpose(u0.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(u0.detach(), 0, 1)[3].view(-1,1)\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  u0[[i]], range(4))\n",
    "    # take derivatives\n",
    "    alphar = gradients(alpha, r0)\n",
    "    \n",
    "    loss_r0 = torch.mean(torch.pow(A-1,2)) + torch.mean(torch.pow(alphar,2)) + torch.mean(torch.pow(phi-phi0,2)) + torch.mean(torch.pow(chi,2))\n",
    "    return loss_r0\n",
    "\n",
    "def rmax_loss(umax):\n",
    "    #A = torch.transpose(umax.detach(), 0, 1)[0].view(-1,1)\n",
    "    #alpha = torch.transpose(umax.detach(), 0, 1)[1].view(-1,1)\n",
    "    #chi = torch.transpose(umax.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(umax.detach(), 0, 1)[3].view(-1,1)\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  umax[[i]], range(4))\n",
    "    \n",
    "    loss_rmax = torch.mean(torch.pow(A-1,2)) + torch.mean(torch.pow(alpha-1,2)) + torch.mean(torch.pow(phi,2)) + torch.mean(torch.pow(chi,2))\n",
    "    return loss_rmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a56d1-b9e8-4c90-876e-bae047ac31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(123)\n",
    "\n",
    "# input 1 (x), output 4 (A, alpha, phi, chi), 32 nodes per layer, 3 hidden layers\n",
    "model = FCN(1,4,64,4).to(device)\n",
    "# true omega = 1.03859806273248977959156036376953125\n",
    "#omega = torch.nn.Parameter(1.04*torch.ones(1, requires_grad=True))\n",
    "\n",
    "#optimizer = torch.optim.Adam(list(model.parameters())+[omega],lr=1e-4)\n",
    "omega = torch.nn.Parameter(1.03859*torch.ones(1))\n",
    "optimizer = torch.optim.Adam(list(model.parameters()),lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# true also below\n",
    "#scheduler = StepLR(optimizer, step_size=2000, gamma=1., verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d0292-7f12-4bd0-a871-0ac718e891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 # number of random sampling points\n",
    "\n",
    "epochs = 200000\n",
    "gamma1 = 100.0\n",
    "gamma2 = 100.0\n",
    "# epsilon is for the random x points, not to get the value 0\n",
    "#epsilon = 1.e-4\n",
    "\n",
    "# mass\n",
    "m = torch.sqrt(2*torch.ones(1)).to(device)\n",
    "# phi(x=0)\n",
    "phi0 = torch.ones(1).to(device)*1.5754400252830238e-2\n",
    "# rmax\n",
    "#rmax = 100#*torch.ones(1).to(device)\n",
    "# lists to save things\n",
    "loss_list = []\n",
    "omegas = []\n",
    "RMAX = 20\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    # x=0\n",
    "    r0 = torch.zeros(1, requires_grad=True).to(device)\n",
    "    # xmax\n",
    "    rmax = RMAX*torch.ones(1, requires_grad=True).to(device)\n",
    "    # time sample\n",
    "    r = RMAX*random_domain_points(n).to(device)\n",
    "    u = model(r)\n",
    "    # loss for the bulk of the domain\n",
    "    loss_dom = domain_loss(u, r, omega, m)\n",
    "    # boundary data at x=0\n",
    "    u0 = model(r0)\n",
    "    loss_r0 = r0_loss(u0, r0, phi0)\n",
    "    # boundary data at x=xmax\n",
    "    umax = model(rmax)\n",
    "    loss_rmax = rmax_loss(umax)\n",
    "    # LOSS\n",
    "    loss = loss_dom + gamma1*loss_r0 + gamma2*loss_rmax \n",
    "    # save loss and omega values\n",
    "    loss_list.append(loss.cpu().detach().numpy())\n",
    "    omegas.append(omega.item())\n",
    "    # print message\n",
    "    print('epoch = ', epoch, '| loss = ', loss.item(), ' | omega = ', omega.item(), '|',  end='\\r')\n",
    "    # detach() removes the \"requires_grad\" and numpy() makes it a numpy item to plot later\n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab412465-6fd0-4ede-84f7-858473d87f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6377023-7586-4ad9-9e43-c29495533f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(omegas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98c7fd-26ed-4bdc-b1a0-a6e3b5c387b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time domain; used to visualize the analytical solution\n",
    "N = 500\n",
    "R = torch.linspace(0,RMAX,N)#.view(-1,10)\n",
    "#print(t)\n",
    "\n",
    "# the view(-1,1) make the row tensor to a column tensor\n",
    "# -1 means you dont know how many rows you need, and 1 mean that you want 1 element in each row\n",
    "RR = torch.linspace(0,RMAX,N).view(-1,1).to(device)\n",
    "\n",
    "nn_sol = model(RR).cpu().detach().numpy() # detach some extra info, and numpy makes a numpy array to plot\n",
    "A_nn, alpha_nn, chi_nn, phi_nn = map(lambda i:  nn_sol[:,[i]], range(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db950b1d-4f33-4a81-934d-eaff97df5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R, A_nn, label=\"A_nn\")\n",
    "plt.xlabel(\"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17bbd5-000d-4d8c-b39e-b9b33f7421c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R, alpha_nn, label=\"alpha_nn\")\n",
    "plt.xlabel(\"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4c5b3-2f89-4b24-885c-fac96929d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R, chi_nn, label=\"chi_nn\")\n",
    "\n",
    "plt.xlabel(\"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146de91-28b9-4345-8d8b-e89108c15124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R, phi_nn, label=\"phi_nn\")\n",
    "\n",
    "plt.xlabel(\"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0171cc-b925-4932-bf95-b7dc38adcc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
